install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
preds.rpart = predict(model.rpart,newdata = test,type = "class")
model.rpart = rpart(y ~ . ,data =train)
preds.rpart = predict(model.rpart,newdata = test,type = "class")
library(rpart)
library(rpart)
model.rpart = rpart(y ~ . ,data =train)
preds.rpart = predict(model.rpart,newdata = test,type = "class")
CrossTable(test$y,preds.rpart,chisq = F,prop.r = F,prop.c = F,prop.t = F,prop.chisq = F)
library(rpart)
model.rpart = rpart(y ~ . ,data =train)
preds.rpart = predict(model.rpart,newdata = test,type = "class")
install.packages("rpart.plot")
source('~/Statistisk evaluering/Des_Tree.r', echo=TRUE)
install.packages("gmodels")
source('~/Statistisk evaluering/Des_Tree.r', echo=TRUE)
install.packages("e1071")
source('~/Statistisk evaluering/Des_Tree.r', echo=TRUE)
install.packages("randomForest")
source('~/Statistisk evaluering/Des_Tree.r', echo=TRUE)
set.seed(100)
model.rf = randomForest(y ~ .,data = train)
preds.rf = predict(model.rf,newdata = test)
CrossTable(preds.rf,test$y,chisq = F,prop.r = F,prop.c = F,prop.t = F,prop.chisq = F)
set.seed(100)
model.rf = randomForest(y ~ .,data = train)
preds.rf = predict(model.rf,newdata = test)
CrossTable(preds.rf,test$y,chisq = F,prop.r = F,prop.c = F,prop.t = F,prop.chisq = F)
source('~/Statistisk evaluering/Des_Tree.r', echo=TRUE)
source('~/Statistisk evaluering/Des_Tree.r', echo=TRUE)
temp = as.data.frame(scale(data[,2:301]))
temp$y = data$y
print(temp)
smp_size =  50
set.seed(123)
train_ind = sample(seq_len(nrow(temp)), size = smp_size)
train = temp[train_ind, ]
test = temp[-train_ind, ]
nrow(train)
nrow(test)
table(train$y)
table(test$y)
set.seed(100)
model.rf = randomForest(y ~ .,data = train)
preds.rf = predict(model.rf,newdata = test)
CrossTable(preds.rf,test$y,chisq = F,prop.r = F,prop.c = F,prop.t = F,prop.chisq = F)
table(train$y)
temp = as.data.frame(scale(data[,2:301]))
temp$y = data$y
print(temp)
smp_size =  100
set.seed(123)
train_ind = sample(seq_len(nrow(temp)), size = smp_size)
train = temp[train_ind, ]
test = temp[-train_ind, ]
nrow(train)
nrow(test)
smp_size =  90
set.seed(123)
train_ind = sample(seq_len(nrow(temp)), size = smp_size)
train = temp[train_ind, ]
test = temp[-train_ind, ]
nrow(train)
nrow(test)
table(train$y)
table(test$y)
set.seed(100)
model.rf = randomForest(y ~ .,data = train)
preds.rf = predict(model.rf,newdata = test)
CrossTable(preds.rf,test$y,chisq = F,prop.r = F,prop.c = F,prop.t = F,prop.chisq = F)
preds.rf
preds.rf = predict.randomforest(model.rf,newdata = test)
preds.rf = predict.randomForest(model.rf,newdata = test)
predict(iris.rf, iris[ind == 2,], predict.all=TRUE)
predict(model.rf, temp[ind == 2,], predict.all=TRUE)
ind <- sample(2, nrow(temp), replace = TRUE, prob=c(0.8, 0.2))
predict(model.rf, temp[ind == 2,], predict.all=TRUE)
table(observed = temp[ind==2, "y"], predicted = preds.rf)
pred.pred <- predict(model.rf, data[ind == 2,])
table(observed = temp[ind==2, "y"], predicted = preds.rf)
table(observed = temp[ind==2, "y"], predicted = preds.pred)
model.rf <- randomForest(y ~ ., data=temp[ind == 1,])
pred.pred <- predict(model.rf, data[ind == 2,])
table(observed = temp[ind==2, "y"], predicted = preds.pred)
table(observed = temp[ind==2, "y"], predicted = pred.pred)
model.rf <- randomForest(y ~ ., data=data[ind == 1,])
pred.pred <- predict(model.rf, data[ind == 2,])
table(observed = temp[ind==2, "y"], predicted = pred.pred)
table(observed = data[ind==2, "y"], predicted = pred.pred)
d <- armdata[[3]]
print(d)
x0 <-c()
for (i in 1:10){
for (j in 1:10){
for (k in 1:3){
x0 <- c(x0, d[[i]][[j]][,k])
}
}
}
x0 <- matrix(x0,nrow=300, byrow=F)
y <- rep(1:10, each=10)
x0 <- data.frame(t(x0))
#trans <- t(x)
#y <- rep(1:10, times=10, each=3)
data <- cbind(y,x0)
#Tjek for rows without data
any(is.na(x0))
#Generate a random number that is 90% of the total number of rows in dataset
set.seed(122)
ran <- sample(1:nrow(data), 0.9 * nrow(data))
#Normalization function
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
#Running normalization function on the 300 columns
data_norm <- as.data.frame(lapply(data[,c(2:301)], nor))
summary(data_norm)
data$y
sample = sample.split(data$y, SplitRatio = .75)
library(randomForest)
require(caTools)
install.packages(caTools)
sample = sample.split(data$y, SplitRatio = .75)
train = subset(data, sample == TRUE)
install.packages('caTools')
library(randomForest)
require(caTools)
sample = sample.split(data$y, SplitRatio = .75)
train = subset(data, sample == TRUE)
test  = subset(data, sample == FALSE)
dim(train)
dim(test)
train = subset(data, sample == TRUE)
test  = subset(data, sample == FALSE)
dim(train)
dim(test)
View(train)
View(test)
dim(train)
dim(test)
rf <- randomForest(
num ~ .,
data=train
)
rf <- randomForest(
y ~ .,
data=train
)
rf
pred = predict(rf, newdata=test[-301])
pred = predict(rf, newdata=test[-300])
pred = predict(rf, newdata=test[-100])
pred = predict(rf, newdata=test[-14])
set.seed(100)
train <- sample(nrow(data), 0.7*nrow(data), replace = FALSE)
TrainSet <- data[train,]
ValidSet <- data[-train,]
summary(TrainSet)
summary(ValidSet)
model1 <- randomForest(y ~ ., data = TrainSet, importance = TRUE)
model1
model2 <- randomForest(y ~ ., data = TrainSet, ntree = 500, mtry = 6, importance = TRUE)
model2
# Predicting on train set
predTrain <- predict(model2, TrainSet, type = "class")
# Checking classification accuracy
table(predTrain, TrainSet$Condition)
# Checking classification accuracy
table(predTrain, TrainSet$y)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
KNN.CV(data,cl,constrain,kn=10)
install.packages('KODAMA')
KNN.CV(data,cl,constrain,kn=10)
library(KODAMA)
KNN.CV(data,cl,constrain,kn=10)
u=data[,-1]
class=as.factor(iris[,1])
results=KNN.CV(u,class,1:length(class))
levels(results)=levels(class)
table(results,class)
library(KODAMA)
#data(iris)
data(iris)
force(iris)
View(iris)
data=iris[,-5]
View(data)
labels=iris[,5]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels
table(pp$Ypred,labels)
data(iris)
data=iris[,-5]
labels=iris[,5]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
d <- armdata[[3]]
print(d)
x0 <-c()
for (i in 1:10){
for (j in 1:10){
for (k in 1:3){
x0 <- c(x0, d[[i]][[j]][,k])
}
}
}
x0 <- matrix(x0,nrow=300, byrow=F)
y <- rep(1:10, each=10)
x0 <- data.frame(t(x0))
#trans <- t(x)
#y <- rep(1:10, times=10, each=3)
data <- cbind(y,x0)
#Tjek for rows without data
any(is.na(x0))
data=data[,2:301]
labels=iris[,1]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
load("~/Statistisk evaluering/StatiskEvaluering_3ugers/armdata.RData")
d <- armdata[[3]]
print(d)
x0 <-c()
for (i in 1:10){
for (j in 1:10){
for (k in 1:3){
x0 <- c(x0, d[[i]][[j]][,k])
}
}
}
x0 <- matrix(x0,nrow=300, byrow=F)
y <- rep(1:10, each=10)
x0 <- data.frame(t(x0))
#trans <- t(x)
#y <- rep(1:10, times=10, each=3)
data <- cbind(y,x0)
#Tjek for rows without data
any(is.na(x0))
data=data[,2:301]
labels=iris[,1]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
D
data=data[,2:301]
labels=iris[,1]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
data=data[,2:301]
labels=data[,1]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
data[,2:301]
data[,2]
data[,2:20]
View(data)
data <- cbind(y,x0)
data <- cbind(y,x0)
data[,2:301]
data=data[,2:301]
labels=data[,1]
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
pp=knn.double.cv(data,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
data[,1]
y <- rep(1:10, each=10)
data <- cbind(y,x0)
dat=data[,2:301]
labels=data[,1]
pp=knn.double.cv(dat,labels)
print(pp$Q2Y)
table(pp$Ypred,labels)
data[,1]
load("~/Statistisk evaluering/StatiskEvaluering_3ugers/armdata.RData")
d <- armdata[[3]]
print(d)
x0 <-c()
for (i in 1:10){
for (j in 1:10){
for (k in 1:3){
x0 <- c(x0, d[[i]][[j]][,k])
}
}
}
x0 <- matrix(x0,nrow=300, byrow=F)
y <- rep(1:10, each=10)
x0 <- data.frame(t(x0))
#trans <- t(x)
#y <- rep(1:10, times=10, each=3)
data <- cbind(x0,y)
#Tjek for rows without data
any(is.na(x0))
View(data)
library(ISLR)
library(caret)
install.packages(ISLR)
install.packages('ISLR')
library(ISLR)
library(caret)
set.seed(300)
#Spliting data as training and test set. Using createDataPartition() function from caret
indxTrain <- createDataPartition(y = data$y,p = 0.75,list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
#Checking distibution in origanl data and partitioned data
prop.table(table(training$y)) * 100
trainX <- training[,names(training) != "Y"]
preProcValues <- preProcess(x = trainX,method = c("center", "scale"))
preProcValues
#Training and control
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)
knnFit <- train(y ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
#Output of kNN fit
knnFit
#Plottin the fit
plot(knnFit)
#Plottin the fit
plot(knnFit, main = 'K-Nearest Neighbor')
confusionMatrix(knnPredict, testing$y)
knnPredict <- predict(knnFit,newdata = testing )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict, testing$y)
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
print(testing)
testing$y
knnPredict <- predict(knnFit,newdata = testing )
knnPredict
len(knnPredict)
length(knnPredict)
length(testing$y)
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict, testing$y)
round(knnPredict)
knn-ny <- round(knnPredict)
knnny <- round(knnPredict)
confusionMatrix(knnny, testing$y)
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(table(knnPredict, testing$y))
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
table(knnPredict, testing$y)
table(round(knnPredict), testing$y)
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(table(round(knnPredict), testing$y))
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
tab <- table(round(knnPredict), testing$y)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
#Accuracy -> Number of true prediction among all predictions
accuracy <- 3/24
accuracy
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(table(round(knnPredict), testing$y))
tab <- table(round(knnPredict), testing$y)
print(tab)
print(indxTrain)
print(training)
print(testing)
print(testing)
knnPredict <- predict(knnFit,newdata = testing )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(table(round(knnPredict), testing$y))
u <- union(knnPredict, testing$y)
t <- table(factor(knnPredict, u), factor(testing$y, u))
confusionMatrix(t)
u <- union(round(knnPredict), testing$y)
t <- table(factor(round(knnPredict), u), factor(testing$y, u))
confusionMatrix(t)
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
mean(knnPredict == testing$y)
ctrl <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,summaryFunction = twoClassSummary)
knnFit <- train(y ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
source('~/.active-rstudio-document', echo=TRUE)
rfFit
library(randomForest)
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)
# Random forest
rfFit <- train(y ~ ., data = training, method = "rf", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
rfFit
plot(rfFit)
rfFit <- train(y ~ ., data = training, method = "rf", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
rfFit
plot(rfFit)
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)
# Random forest
rfFit <- train(data$y ~ ., data = training, method = "rf", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
rfFit
plot(rfFit)
# Random forest
rfFit <- train(y ~ ., data = training, method = "rf", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
rfFit
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
source('~/Statistisk evaluering/Projekt_hygge.R', echo=TRUE)
library(class)
library(ISLR)
library(caret)
pr <- c()
tab <- c()
a <- c()
for(i in 1:90){
pr <- knn(data_train,data_test,cl=data_target_category,k=i)
pr
u <- union(pr, data_test_category)
t <- table(factor(pr, u), factor(data_test_category, u))
confusionMatrix(t)
tab <- table(pr,data_test_category)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
a <- c(a, accuracy(tab))
}
print(pr)
print(tab)
print(a)
library(class)
library(ISLR)
library(caret)
pr <- c()
tab <- c()
a <- c()
for(i in 1:90){
pr <- knn(data_train,data_test,cl=data_target_category,k=i)
pr
u <- union(pr, data_test_category)
t <- table(factor(pr, u), factor(data_test_category, u))
confusionMatrix(t)
tab <- table(pr,data_test_category)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
a <- c(a, accuracy(tab))
}
print(pr)
print(tab)
print(a)
print(t)
library(class)
library(ISLR)
library(caret)
pr <- c()
tab <- c()
a <- c()
for(i in 1:90){
pr <- knn(data_train,data_test,cl=data_target_category,k=i)
pr
u <- union(pr, data_test_category)
t <- table(factor(pr, u), factor(data_test_category, u))
confusionMatrix(t)
tab <- table(pr,data_test_category)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
a <- c(a, accuracy(t))
}
print(pr)
print(tab)
print(a)
print(t)
library(class)
library(ISLR)
library(caret)
pr <- c()
tab <- c()
a <- c()
for(i in 1:90){
pr <- knn(data_train,data_test,cl=data_target_category,k=i)
pr
u <- union(pr, data_test_category)
t <- table(factor(pr, u), factor(data_test_category, u))
cm <- confusionMatrix(t)
tab <- table(pr,data_test_category)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
a <- c(a, accuracy(t))
}
print(pr)
print(tab)
print(a)
print(t)
print(cm)
library(class)
library(ISLR)
library(caret)
pr <- c()
tab <- c()
a <- c()
for(i in 1:90){
pr <- knn(data_train,data_test,cl=data_target_category,k=i)
pr
u <- union(pr, data_test_category)
t <- table(factor(pr, u), factor(data_test_category, u))
tab <- table(pr,data_test_category)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
a <- c(a, accuracy(t))
}
print(pr)
print(tab)
print(a)
print(t)
source('~/Statistisk evaluering/Cross-val-of-KNN.R', echo=TRUE)
source('~/Statistisk evaluering/Cross-val-of-RF.R', echo=TRUE)
